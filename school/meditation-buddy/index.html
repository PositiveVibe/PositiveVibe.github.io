<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Meditation Buddy # Summary # Meditation Buddy was my thesis for Tsinghua University. I was the creator of the project and saw it through from initial conception to launching and testing with 20 different participants to see the effectiveness of the app.
Meditation Buddy is a meditation app that uses the smartphone camera to assess the user&rsquo;s body position of the upper torso and head. Based on their body position for the last 6 seconds, an inference is made of whether or not they are meditating."><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:title" content="Meditation Buddy"><meta property="og:description" content="Meditation Buddy # Summary # Meditation Buddy was my thesis for Tsinghua University. I was the creator of the project and saw it through from initial conception to launching and testing with 20 different participants to see the effectiveness of the app.
Meditation Buddy is a meditation app that uses the smartphone camera to assess the user&rsquo;s body position of the upper torso and head. Based on their body position for the last 6 seconds, an inference is made of whether or not they are meditating."><meta property="og:type" content="article"><meta property="og:url" content="/school/meditation-buddy/"><meta property="article:section" content="school"><title>Meditation Buddy |</title><link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.60f8a511f82784c81cfc544a62a8a06a51918f41b2764ab2d3b396543e43b5b8.css></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><img src=/logo.png alt=Logo><span></span></a></h2><ul><li><input type=checkbox id=section-ab34ffd594ecc88a9cd4fbf66778b163 class=toggle checked>
<label for=section-ab34ffd594ecc88a9cd4fbf66778b163 class="flex justify-between"><a role=button>School</a></label><ul><li><a href=/school/meditation-buddy/ class=active>Meditation Buddy</a></li><li><a href=/school/battgenie/>BatteREuse</a></li><li><a href=/school/boombox-2/>Boombox 2.0</a></li><li><a href=/school/deepfake-detection/>Deepfake Detection</a></li><li><a href=/school/align/>Undergrad</a></li></ul></li><li><input type=checkbox id=section-ad115dd6f6c5b4603ba83668130ab788 class=toggle>
<label for=section-ad115dd6f6c5b4603ba83668130ab788 class="flex justify-between"><a role=button>Startups</a></label><ul><li><a href=/startups/emc/>Ethereum Mining Coalition</a></li><li><a href=/startups/rngs-for-trees/>RNGs for Trees</a></li><li><a href=/startups/d3sg/>Diablo 3 Strategy Guide</a></li><li><a href=/startups/budshark/>Budshark</a></li></ul></li><li><input type=checkbox id=section-1752ede2de78bab074826c342a8796ed class=toggle>
<label for=section-1752ede2de78bab074826c342a8796ed class="flex justify-between"><a role=button>Work</a></label><ul><li><a href=/work/skyrock/>Skyrock Projects</a></li><li><a href=/work/atg/>Accretive Technology Group</a></li><li><a href=/work/alltreatment/>AllTreatment</a></li><li><a href=/work/uw/>University of Washington</a></li><li><a href=/work/boeing/>Boeing</a></li><li><a href=/work/consulting/>Consulting</a></li></ul></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu></label>
<strong>Meditation Buddy</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#summary>Summary</a></li><li><a href=#problem-space>Problem Space</a><ul><li></li></ul></li><li><a href=#proposed-solution>Proposed Solution</a><ul><li></li></ul></li><li><a href=#data-collection>Data Collection</a><ul><li></li></ul></li><li><a href=#building-the-model>Building the Model</a></li><li><a href=#evaluating-the-model>Evaluating the Model</a><ul><li><a href=#processing-data>Processing Data</a></li><li><a href=#building-the-app>Building the App</a></li></ul></li><li><a href=#user-tests-and-future-work>User Tests and Future Work</a></li></ul></nav></aside></header><article class=markdown><h1 id=meditation-buddy>Meditation Buddy
<a class=anchor href=#meditation-buddy>#</a></h1><h2 id=summary>Summary
<a class=anchor href=#summary>#</a></h2><p>Meditation Buddy was my thesis for Tsinghua University. I was the creator of the project and saw it through from initial conception to launching and testing with 20 different participants to see the effectiveness of the app.</p><p>Meditation Buddy is a meditation app that uses the smartphone camera to assess the user&rsquo;s body position of the upper torso and head. Based on their body position for the last 6 seconds, an inference is made of whether or not they are meditating. If they deemed &ldquo;distacted&rdquo; for 21 seconds, then a gong sound would play to redirect their attention. Meditations would last 10 or 20 minutes.</p><p>I really enjoyed this project, as I was able to combine some things I learned in yoga with technology. I&rsquo;ve been a yoga practitioner for 10+ years (and a yoga teacher for 6+ years). So it was really exciting for me to take what I learned about computer vision, data science, and development and combine it with meditiation - one of yoga&rsquo;s tools.</p><h2 id=problem-space>Problem Space
<a class=anchor href=#problem-space>#</a></h2><p>Meditation is a common practice method that can effectively help users combat psychological stress. One key challenge for new meditators is to regain mindfulness when their minds start wandering.</p><h4 id=the-monkey-mind>The Monkey Mind
<a class=anchor href=#the-monkey-mind>#</a></h4><p>A common metaphor within the meditation world is that our minds are like monkeys, swinging from thought to thought[5]. Through meditation, we can stop the monkey swinging from the next thought, instead focusing on a single thought while avoiding the distractions that come up. Through continued practice we will be able to better tame the monkey that is our mind.</p><p>While there are many different types of meditation, almost all of them involve focusing the mind on an object of focus, like a sensation in the body, a sound, or thought. In these meditations, the act of med- itation is when the meditator notices their mind is wandering (or that the monkey mind has swung to another branch), and then brings their attention back to the object of focus. Experienced meditators are usually better at noticing when their mind wanders than new meditators [6]</p><h4 id=how-to-stop-that-monkey>How to Stop that Monkey
<a class=anchor href=#how-to-stop-that-monkey>#</a></h4><p>For new meditators, it is often a challenge[7]. Distractions, questions about the technique, and understanding whether or not they are doing it correctly are all challenges new meditators face. There are also a variety of methods that meditators try when attempting meditation early on in their meditation career. This includes in-person with a meditation instructor, through an online app that guides them through a meditation, and brain-computer interface (BCI) devices such as the Muse or Neurosky headsets, which measure EEG waves to determine the state of the brain.</p><p><figure><img src=/img/school/mbuddy1.png alt="Example of (A) the Muse headset, (B) Neurosky Mindwave, and a (C) medical-grade
EEG."><figcaption>Example of (A) the Muse headset, (B) Neurosky Mindwave, and a (C) medical-grade
EEG.</figcaption></figure></p><p>Of these methods, only the instructor and BCI methods allow for feedback and clarifying of technique to know they are doing it correctly. However, these methods require either inordinate time or a significant amount of money to achieve. The app method, however, is very accessible. This has some drawbacks, however, in that they cannot get feedback to know if they are doing the technique correctly.</p><p><figure><img src=/img/school/mbuddy103.png alt="Can there be a contactless way to merge these solutions?"><figcaption>Can there be a contactless way to merge these solutions?</figcaption></figure></p><h2 id=proposed-solution>Proposed Solution
<a class=anchor href=#proposed-solution>#</a></h2><p>A non-contact method for determining meditation level and giving feedback is proposed. In this solution, no BCI devices are needed in order to detect the meditation level, allowing for the accessibility of an online app while providing the advantages of a BCI device. Instead, a video stream is processed to determine the relevant keypoints of the body, which is then sent to a machine learning model specifically built for the task of determining brain-states based on the relevant keypoints.</p><p><figure><img src=/img/school/mbuddy101.png alt="Proposed solution - the meditator uses a smartphone to give realtime feedback of their meditative state."><figcaption>The meditator sits in front of the smartphone camera, features are extracted. Those features inform a prediction of focus vs distraction. Based on the classification, it may give a gentle reminder to the user that they need to focus on their breath.</figcaption></figure></p><h4 id=posenet-and-finding-keypoints>Posenet and Finding Keypoints
<a class=anchor href=#posenet-and-finding-keypoints>#</a></h4><p>Posenet[34][35] is a machine learning model that takes in a video or image and give a 2-D pose estimation of any people within the frame. It has been further improved upon by the Google Brain team, and integrated into TensorFlow.js library[36].</p><p><figure><img src=/img/school/mbuddy102.png alt="14 pieces of data - x, y location of nose, eyes, ears, shoulders."><figcaption>14 pieces of data - x, y location of nose, eyes, ears, shoulders.</figcaption></figure></p><p>Posenet has been utilized in a number of research areas, like improving the swing for golfers[37] and recognizing human activity[38]. Posenet is chosen for this research because of the reliability and for the speed at which it can make predictions about the human body within the image. Additionally, with the integration into the TensorFlow.js library, it is a good candidate to get working on a smartphone as the documentation and support for TensorFlow.js is reliable.</p><h2 id=data-collection>Data Collection
<a class=anchor href=#data-collection>#</a></h2><p>In order to make prediction of the user&rsquo;s brain state based on video data, I needed to collect many different users meditating in front of a camera while wearing the Muse headset. The Muse headset allowed me to gather a &ldquo;golden truth&rdquo; of the meditation status of the users.</p><p><figure><img src=/img/school/mbuddy200.png alt="Example frame from the meditation session. Participant is wearing the Muse headset while being guiding through a 10 minute breathing meditation."><figcaption>Example frame from the meditation session. Participant is wearing the Muse headset while being guiding through a 10 minute breathing meditation. Shoutout to the all the participants. Thank you!!</figcaption></figure></p><p>12 participants with varying degrees of meditation experience were recruited (5 male and 7 female). 10 of the participants had little to no experience with meditation.</p><h4 id=brainwaves-and-relative-alpha>Brainwaves and Relative Alpha
<a class=anchor href=#brainwaves-and-relative-alpha>#</a></h4><p>Different patterns of brainwaves can be recognized by their amplitudes and frequencies. Brainwaves can then be categorized based on their level of activity or frequency.</p><p>At any given time, our brain is giving off all of these waves. Many studies have looked into the ratio and strength of the waves while meditating. Numerous studies[19][28][29][30][31][32] have noted that there is a higher relative alpha (sometimes called alpha power) while in a state of meditation. This means that For instance, a re- view of the studies involving EEG and meditation by Cahn et al. found that ‚ÄùAlpha power increases are often observed when meditators are evaluated during meditating compared with control conditions.‚Äù[28]</p><p>In order to get a more accurate picture of when people were meditating, a function to find the relative Alpha waves throughout the meditation was identified and used. This formula is used by the Muse headset creators[39] to find the power of the Alpha waves compared to the other waves in the brain.</p><p>Relative Alpha = (10ùõº )/(10ùõº + 10ùõΩ + 10ùõø + 10ùõæ + 10ùúÉ )</p><p>This formula outputs a value between 0 and 1, when it is closer to 1 the Alpha waves are most dominant and the user can be inferred to be in a meditative state.</p><p><figure><img src=/img/school/mbuddy202.png alt="Participant 1, an example of meditating for most of the session. Relative alpha is shown in orange."><figcaption>Participant 1, an example of meditating for most of the session. Relative alpha is shown in orange.</figcaption></figure></p><p><figure><img src=/img/school/mbuddy201.png alt="Participant 2, an example of distraction for most of the session. Relative alpha is shown in orange."><figcaption>Participant 2, an example of distraction for most of the session. Relative alpha is shown in orange.</figcaption></figure></p><p>At this point, with the graphed data showing the brainwaves over time and the relative alpha of those brainwaves, the next step was to go to the video and clip the different moments when the user was in high and low relative alpha, in order to create clips of meditation and distraction, respectively.</p><h4 id=labeling-procedure>Labeling Procedure
<a class=anchor href=#labeling-procedure>#</a></h4><ol><li>Identify areas in the graph that showed either focus or distraction through relative alpha power at 98% or above.</li><li>Find the corresponding time in the video.</li><li>Cut a series of new 6-second video clips of the participant during
this time, 3 seconds apart.</li><li>Save the data in the corresponding distracted or focused folder.</li></ol><p>Each video is cut into 6 second clips every 3 seconds, to create overlapping windows of video data. The aim of this overlapping window technique is to create more data for the machine learning algorithm later, as using discrete clips with no overlap produced less data than originally hoped for. 6 second clips were chosen because if the clips were too short, then we wouldn‚Äôt be able to get an accurate idea of the user state. If the clips were too long, then we wouldn‚Äôt be able to give feedback quickly enough to the user.</p><p>Using this formula, the clips for focused meditation were taken when there was a clear streak of high alpha (relative alpha at or above 98%), and the distracted clips were taken when there were clear drops in relative alpha or streak with relative alpha below 80%. In total, <strong>149 focused meditation clips</strong> and <strong>142 distracted clips</strong> were obtained.</p><h2 id=building-the-model>Building the Model
<a class=anchor href=#building-the-model>#</a></h2><p><figure><img src=/img/school/mbuddy302.png alt="Table showing the origin of the different features and which signals are extracted."><figcaption>Table showing the origin of the different features and which signals are extracted.
The top 14 features with the highest information gain were selected. The number of signals involved is only 4.</figcaption></figure></p><p><figure><img src=/img/school/mbuddy301.png alt="Architecture of the machine learning model. Starting with the 14 key position coordinates, ultimately 14 features with the highest information gain were chosen to be included in the Random Forest model."><figcaption>Architecture of the machine learning model. Starting with the 14 key position coordinates, ultimately 14 features with the highest information gain were chosen to be included in the Random Forest model.</figcaption></figure></p><h2 id=evaluating-the-model>Evaluating the Model
<a class=anchor href=#evaluating-the-model>#</a></h2><h3 id=processing-data>Processing Data
<a class=anchor href=#processing-data>#</a></h3><h3 id=building-the-app>Building the App
<a class=anchor href=#building-the-app>#</a></h3><p><figure><img src=/img/school/mbuddy401.png alt="Data Flow between User, App, and the Server running the Machine Learning Model."><figcaption>Data Flow between User, App, and the Server running the Machine Learning Model.</figcaption></figure></p><p>Mortalia rudibusque caelum cognosceret tantum aquis redito felicior texit, nec,
aris parvo acre. Me parum contulerant multi tenentem, gratissime suis; vultum tu
occupat deficeret corpora, sonum. E Actaea inplevit Phinea concepit nomenque
potest sanguine captam nulla et, in duxisses campis non; mercede. Dicere cur
Leucothoen obitum?</p><p>Postibus mittam est <em>nubibus principium pluma</em>, exsecratur facta et. Iunge
Mnemonidas pallamque pars; vere restitit alis flumina quae <strong>quoque</strong>, est
ignara infestus Pyrrha. Di ducis terris maculatum At sede praemia manes
nullaque!</p><h2 id=user-tests-and-future-work>User Tests and Future Work
<a class=anchor href=#user-tests-and-future-work>#</a></h2><p>Machine Learning, App Development, Neuroscience</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){if(window.getSelection().toString())return;e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><div class=book-comments></div><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#summary>Summary</a></li><li><a href=#problem-space>Problem Space</a><ul><li></li></ul></li><li><a href=#proposed-solution>Proposed Solution</a><ul><li></li></ul></li><li><a href=#data-collection>Data Collection</a><ul><li></li></ul></li><li><a href=#building-the-model>Building the Model</a></li><li><a href=#evaluating-the-model>Evaluating the Model</a><ul><li><a href=#processing-data>Processing Data</a></li><li><a href=#building-the-app>Building the App</a></li></ul></li><li><a href=#user-tests-and-future-work>User Tests and Future Work</a></li></ul></nav></div></aside></main></body></html>